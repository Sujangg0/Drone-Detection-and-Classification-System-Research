{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a8a61ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-13 02:26:03.890020: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/ghoth/Programming/Python_environment/lib/python3.11/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PYTORCH_CUDA_ALLOC_CONF set to: expandable_segments:True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchaudio.transforms import Spectrogram\n",
    "import lib.model_VGG2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.backends.cudnn as cudnn\n",
    "from tqdm import tqdm  # progressbar\n",
    "import torchmetrics\n",
    "import pickle as pkl\n",
    "\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "print(\"PYTORCH_CUDA_ALLOC_CONF set to:\", os.environ.get(\"PYTORCH_CUDA_ALLOC_CONF\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33994a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class drone_data_dataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for drone IQ Signals + transform to spectrogram\n",
    "    \"\"\"\n",
    "    def __init__(self, path, transform=None, device=None):\n",
    "        self.path = path\n",
    "        self.files = os.listdir(path)\n",
    "        self.files = [f for f in self.files if f.endswith('pt')] # filter for files with .pt extension  \n",
    "        self.files = [f for f in self.files if f.startswith('IQdata_sample')] # filter for files which start with IQdata_sample in name\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "\n",
    "        # create list of tragets and snrs for all samples\n",
    "        self.targets = []\n",
    "        self.snrs = []\n",
    "        \n",
    "        for file in self.files:\n",
    "            self.targets.append(int(file.split('_')[2][6:])) # get target from file name\n",
    "            self.snrs.append(int(file.split('_')[3].split('.')[0][3:])) # get snr from file name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file = self.files[idx]\n",
    "        sample_id = int(file.split('_')[1][6:]) # get sample id from file name\n",
    "        data_dict = torch.load(self.path + file) # load data       \n",
    "        iq_data = data_dict['x_iq']\n",
    "        act_target = data_dict['y']\n",
    "        act_snr = data_dict['snr']\n",
    "\n",
    "        if self.transform:\n",
    "            if self.device:\n",
    "                iq_data = iq_data.to(device=device)\n",
    "            transformed_data = self.transform(iq_data)\n",
    "        else:\n",
    "            transformed_data = None\n",
    "\n",
    "        return iq_data, act_target, act_snr, sample_id, transformed_data\n",
    "    \n",
    "    def get_targets(self): # return list of targets\n",
    "        return self.targets\n",
    "\n",
    "    def get_snrs(self): # return list of snrs\n",
    "        return self.snrs\n",
    "    \n",
    "    def get_files(self):\n",
    "        return self.files\n",
    "\n",
    "\n",
    "class transform_spectrogram(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        device,\n",
    "        n_fft=512,          # Change from 1024 to 512\n",
    "        win_length=512,     # Same\n",
    "        hop_length=512,     # Or try 256 for more overlap/time resolution\n",
    "        window_fn=torch.hann_window,\n",
    "        power=None, # Exponent for the magnitude spectrogram, (must be > 0) e.g., 1 for magnitude, 2 for power, etc. If None, then the complex spectrum is returned instead. (Default: 2)\n",
    "        normalized=False,\n",
    "        center=False,\n",
    "        #pad_mode='reflect',\n",
    "        onesided=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.spec = Spectrogram(n_fft=n_fft, win_length=win_length, hop_length=hop_length, window_fn=window_fn, power=power, normalized=normalized, center=center, onesided=onesided).to(device=device)   \n",
    "        self.win_lengt = win_length\n",
    "\n",
    "    def forward(self, iq_signal: torch.Tensor) -> torch.Tensor:\n",
    "        # Convert to spectrogram\n",
    "        iq_signal = iq_signal[0,:] + (1j * iq_signal[1,:]) # convert to complex signal\n",
    "        spec = self.spec(iq_signal)\n",
    "        spec = torch.view_as_real(spec) # Returns a view of a complex input as a real tensor. last dimension of size 2 represents the real and imaginary components of complex numbers\n",
    "        spec = torch.moveaxis(spec,2,0) # move channel dimension to first dimension (1024, 1024, 2) -> (2, 1024, 1024)\n",
    "        spec = spec/self.win_lengt # normalise by fft window size\n",
    "        return spec\n",
    "\n",
    "\n",
    "def plot_two_channel_spectrogram(spectrogram_2d, title='', figsize=(10,6)):\n",
    "    figure, axis = plt.subplots(1, 2, figsize=figsize)\n",
    "    re = axis[0].imshow(spectrogram_2d[0,:,:]) #, aspect='auto', origin='lower')\n",
    "    axis[0].set_title(\"Re\")\n",
    "    figure.colorbar(re, ax=axis[0], location='right', shrink=0.5)\n",
    "\n",
    "    im = axis[1].imshow(spectrogram_2d[1,:,:]) #, aspect='auto', origin='lower')\n",
    "    axis[1].set_title(\"Im\")\n",
    "    figure.colorbar(im, ax=axis[1], location='right', shrink=0.5)\n",
    "\n",
    "    figure.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_two_channel_iq(iq_2d, title='', figsize=(10,6)):\n",
    "    figure, axis = plt.subplots(2, 1, figsize=figsize)\n",
    "    axis[0].plot(iq_2d[0,:]) \n",
    "    axis[0].set_title(\"Re\")\n",
    "    axis[1].plot(iq_2d[1,:])\n",
    "    axis[1].set_title(\"Im\")\n",
    "    figure.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def get_model_spec(model_name, num_classes):\n",
    "    if(model_name == 'vgg11'):\n",
    "        return lib.model_VGG2D.vgg11(num_classes=num_classes)\n",
    "    elif(model_name == 'vgg11_bn'):\n",
    "        return lib.model_VGG2D.vgg11_bn(num_classes=num_classes)\n",
    "    elif(model_name == 'vgg13'):\n",
    "        return lib.model_VGG2D.vgg13(num_classes=num_classes)\n",
    "    elif(model_name == 'vgg13_bn'):\n",
    "        return lib.model_VGG2D.vgg13_bn(num_classes=num_classes)\n",
    "    elif(model_name == 'vgg16'):\n",
    "        return lib.model_VGG2D.vgg16(num_classes=num_classes)\n",
    "    elif(model_name == 'vgg16_bn'):\n",
    "        return lib.model_VGG2D.vgg16_bn(num_classes=num_classes)\n",
    "    elif(model_name == 'vgg19'):\n",
    "        return lib.model_VGG2D.vgg19(num_classes=num_classes)\n",
    "    elif(model_name == 'vgg19_bn'):\n",
    "        return lib.model_VGG2D.vgg19_bn(num_classes=num_classes)\n",
    "    else:\n",
    "        print('Error: no valid model name:', model_name)\n",
    "        exit()\n",
    "\n",
    "\n",
    "def train_model_observe_snr_performance_spec(\n",
    "    model, criterion, optimizer, scheduler,\n",
    "    num_classes, num_epochs, snr_list_for_observation,\n",
    "    best_acc=0.0, best_epoch=0, best_model_wts=None\n",
    "):  \n",
    "    since = time.time()\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    train_weighted_acc = []\n",
    "    lr = []\n",
    "\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    val_weighted_acc = []\n",
    "\n",
    "    # create variables to store acc for different SNR samples\n",
    "    num_snrs_to_observe = len(snr_list_for_observation)\n",
    "    \n",
    "    if best_model_wts is None:\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "\n",
    "    print('start training')\n",
    "    print('-' * 10)\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs): # ← IMPORTANT: use start_epoch\n",
    "        # initialize metric\n",
    "        # accuracy\n",
    "        train_metric_acc = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes).to(device)\n",
    "        val_metric_acc = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes).to(device)\n",
    "\n",
    "        # weigthed accuracy\n",
    "        # 'macro': Calculate the metric for each class separately, and average the metrics across classes (with equal weights for each class).\n",
    "        train_metric_weighted_acc = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes, average='macro').to(device)\n",
    "        val_metric_weighted_acc = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes, average='macro').to(device)\n",
    "\n",
    "        # snr dependent accuracies metrics\n",
    "        snr_val_metric_acc_list =[torchmetrics.Accuracy(task='multiclass', num_classes=num_classes).to(device) for i in range(num_snrs_to_observe)]\n",
    "        snr_val_metric_weighted_acc_list =[torchmetrics.Accuracy(task='multiclass', num_classes=num_classes, average='macro').to(device) for i in range(num_snrs_to_observe)]\n",
    "        \n",
    "        # snr dependent accuracies storage for epoch\n",
    "        snr_epoch_acc = torch.zeros([num_snrs_to_observe], dtype=torch.float)\n",
    "        snr_epoch_weighted_acc = torch.zeros([num_snrs_to_observe], dtype=torch.float)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            # phase = 'train'\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "                running_loss = 0.0\n",
    "                epoch_train_loop = tqdm(dataloaders[phase])  # setup progress bar\n",
    "\n",
    "                # iterate over data of the epoch (training)\n",
    "                # inputs, labels, snrs = next(iter(epoch_train_loop))\n",
    "                # for batch_id, (inputs_iq, inputs_spec, labels, snrs, duty_cycles) in enumerate(epoch_train_loop):\n",
    "                # iq_data, target, act_snr, sample_id, transformed_data = next(iter(epoch_train_loop))\n",
    "                for batch_id, (iq_data, target, act_snr, sample_id, transformed_data) in enumerate(epoch_train_loop):\n",
    "                    inputs = transformed_data.to(device)\n",
    "                    labels = target.to(device)\n",
    "                    \n",
    "                    # add model graph to tensorboard\n",
    "                    if (batch_id==0) & (epoch==0):\n",
    "                        # writer.add_graph(model, inputs)  # This alone can cause OOM with large inputs\n",
    "                        pass\n",
    "                    \n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(True):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                    # compute scores for the epoch\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                    # compute scores for batch\n",
    "                    train_metric_acc.update(preds, labels.data)\n",
    "                    train_metric_weighted_acc.update(preds, labels.data)\n",
    "\n",
    "                    # print(f\"Accuracy on batch: {batch_train_acc}\")\n",
    "                    if train_verbose:\n",
    "                        # show progress bar for the epoch\n",
    "                        epoch_train_loop.set_description(f'Epoch [{epoch}/{num_epochs}]')\n",
    "                        # epoch_train_loop.set_postfix(loss=loss.item(), acc=torch.sum(preds == labels.data).item()/batch_size)\n",
    "                        epoch_train_loop.set_postfix()\n",
    "\n",
    "                # apply learning rate scheduler after training epoch (for exp_lr_scheduler)\n",
    "                # scheduler.step()\n",
    "\n",
    "                # compute and show metrics for the epoch\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = train_metric_acc.compute().item()\n",
    "                epoch_weighted_acc = train_metric_weighted_acc.compute().item()\n",
    "\n",
    "                print('{} Loss: {:.4f} Acc: {:.4f}  Balanced Acc: {:.4f} |'.format(phase, epoch_loss, epoch_acc, epoch_weighted_acc), end=' ')\n",
    "\n",
    "                # store metric for epoch\n",
    "                train_loss.append(epoch_loss)\n",
    "                train_acc.append(epoch_acc)\n",
    "                train_weighted_acc.append(epoch_weighted_acc)\n",
    "                lr.append(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "                # add to tensor board\n",
    "                writer.add_scalar('Loss/train', epoch_loss, epoch)\n",
    "                writer.add_scalar('Accuracy/train', epoch_acc, epoch)\n",
    "                writer.add_scalar('BalancedAccuracy/train', epoch_weighted_acc, epoch)\n",
    "                writer.add_scalar('Learnigrate', optimizer.param_groups[0]['lr'], epoch)\n",
    "            else:\n",
    "                # phase = 'val'\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "                running_loss = 0.0\n",
    "\n",
    "                # iterate over data of the epoch (evaluation)\n",
    "                for batch_id, (iq_data, target, act_snr, sample_id, transformed_data) in enumerate(dataloaders[phase]):\n",
    "                    inputs = transformed_data.to(device)\n",
    "                    labels = target.to(device)\n",
    "                    snrs = act_snr.to(device)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    with torch.set_grad_enabled(False):\n",
    "                        outputs = model(inputs)\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                    # compute scores for batch\n",
    "                    val_metric_acc.update(preds, labels.data)\n",
    "                    val_metric_weighted_acc.update(preds, labels.data)\n",
    "\n",
    "                    # compute accuracies for diffrent SNRs\n",
    "                    for i, snr in enumerate(snr_list_for_observation):\n",
    "                        act_snr_sample_indices = torch.where(snrs == snr)[0]\n",
    "                        if act_snr_sample_indices.size(0) > 0: # if there are some samples with current SNR\n",
    "                            snr_val_metric_acc_list[i].update(preds[act_snr_sample_indices], labels.data[act_snr_sample_indices])\n",
    "                            snr_val_metric_weighted_acc_list[i].update(preds[act_snr_sample_indices], labels.data[act_snr_sample_indices])\n",
    "                            \n",
    "                # compute and show metrics for the epoch\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = val_metric_acc.compute().item()\n",
    "                epoch_weighted_acc = val_metric_weighted_acc.compute().item()\n",
    "\n",
    "                for i in range(num_snrs_to_observe):\n",
    "                    snr_epoch_acc[i] = snr_val_metric_acc_list[i].compute().item()\n",
    "                    snr_epoch_weighted_acc[i] = snr_val_metric_weighted_acc_list[i].compute().item()\n",
    "\n",
    "                # apply LR scheduler ... looking for plateau in val loss\n",
    "                if scheduler:\n",
    "                    scheduler.step(epoch_loss)\n",
    "\n",
    "                print('{} Loss: {:.4f} Acc: {:.4f}  Balanced Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc, epoch_weighted_acc))\n",
    "               \n",
    "\n",
    "                # store validation loss\n",
    "                val_loss.append(epoch_loss)\n",
    "                val_acc.append(epoch_acc)\n",
    "                val_weighted_acc.append(epoch_weighted_acc)\n",
    "\n",
    "                # add to tensor board\n",
    "                writer.add_scalar('Loss/val', epoch_loss, epoch)\n",
    "                writer.add_scalar('Accuracy/val', epoch_acc, epoch)\n",
    "                writer.add_scalar('BalancedAccuracy/val', epoch_weighted_acc, epoch)\n",
    "\n",
    "                # SNR measures to tensorboard\n",
    "                for i, snr in enumerate(snr_list_for_observation):\n",
    "                    writer.add_scalar('SNR/val Accuracy SNR' + str(snr), snr_epoch_acc[i], epoch)\n",
    "                    writer.add_scalar('SNR/val BalancedAccuracy SNR' + str(snr), snr_epoch_weighted_acc[i], epoch)\n",
    "\n",
    "                # best_epoch = epoch\n",
    "                if epoch_weighted_acc > best_acc:\n",
    "                    best_acc = epoch_weighted_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    best_epoch = epoch\n",
    "                    \n",
    "                    # SAVE BEST MODEL SEPARATELY\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': best_model_wts,\n",
    "                        'best_acc': best_acc\n",
    "                    }, act_result_path + f'best_model_fold{fold}.pth')\n",
    "                                    \n",
    "                    \n",
    "\n",
    "        # --- SAVE CHECKPOINT EVERY EPOCH ---\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_ft.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'train_acc': train_acc,\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc,\n",
    "            'train_weighted_acc': train_weighted_acc,\n",
    "            'val_weighted_acc': val_weighted_acc,\n",
    "            'best_epoch': best_epoch,\n",
    "            'best_acc': best_acc,          \n",
    "           'lr': lr \n",
    "        }\n",
    "        checkpoint_path = act_result_path + f'checkpoint_fold{fold}_epoch{epoch}.pth'\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "        # --- END SAVE ---\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "    print('Best epoch: {}'.format(best_epoch))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    return model, train_loss, train_acc, val_loss, val_acc, train_weighted_acc, val_weighted_acc, best_epoch, lr\n",
    "\n",
    "\n",
    "def eval_model_spec(model, num_classes, data_loader):\n",
    "    # init tensor to model outputs and targets\n",
    "    eval_targets = torch.empty(0, device=device)\n",
    "    eval_predictions = torch.empty(0, device=device)\n",
    "    \n",
    "    eval_snrs = torch.empty(0, device=device)\n",
    "    eval_duty_cycle = torch.empty(0, device=device)\n",
    "\n",
    "    # initialize metric\n",
    "    eval_metric_acc = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes,).to(device) # accuracy\n",
    "\n",
    "    # 'macro': Calculate the metric for each class separately, and average the metrics across classes (with equal weights for each class).\n",
    "    eval_metric_weighted_acc = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes, average='macro').to(device) # weigthed accuracy\n",
    "\n",
    "    # evaluate the model\n",
    "    model.eval()  # Set model to evaluate mode\n",
    "\n",
    "    # iterate over data of the epoch (evaluation)\n",
    "    for batch_id, (iq_data, target, act_snr, sample_id, transformed_data) in enumerate(data_loader):\n",
    "        inputs = transformed_data.to(device)\n",
    "        labels = target.to(device)\n",
    "        snrs = act_snr.to(device)\n",
    "\n",
    "        # forward through model\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        # store batch model outputs and targets\n",
    "        eval_predictions = torch.cat((eval_predictions, preds.data))\n",
    "        eval_targets = torch.cat((eval_targets, labels.data))\n",
    "        eval_snrs = torch.cat((eval_snrs, snrs.data))\n",
    "        \n",
    "        # compute batch evaluation metric\n",
    "        eval_metric_acc.update(preds, labels.data)\n",
    "        eval_metric_weighted_acc.update(preds, labels.data)\n",
    "\n",
    "    # compute metrics for complete data\n",
    "    eval_acc = eval_metric_acc.compute().item()\n",
    "    eval_weighted_acc = eval_metric_weighted_acc.compute().item()\n",
    "\n",
    "    return eval_acc, eval_weighted_acc, eval_predictions, eval_targets, eval_snrs, eval_duty_cycle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5be9d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment: vgg11_bn_CV5_epochs40_lr0.001_batchsize2\n",
      "[Errno 17] File exists: './results/experiments/vgg11_bn_CV5_epochs50_lr0.001_batchsize2/'\n",
      "[Errno 17] File exists: './results/experiments/vgg11_bn_CV5_epochs50_lr0.001_batchsize2/plots/'\n",
      "Fold: 0\n",
      "Resuming training from checkpoint: /home/ghoth/thesis_drone_detection/Robust-Drone-Detection-and-Classification/results/experiments/vgg11_bn_CV5_epochs50_lr0.001_batchsize2/checkpoint_fold0_epoch39.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ghoth/Programming/Python_environment/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_4192/1317246258.py:159: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(resume_checkpoint, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumed from epoch 39. Starting at epoch 40\n",
      "start training\n",
      "----------\n",
      "Training complete in 0m 0s\n",
      "Best val Acc: 0.000000\n",
      "Best epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4192/1317246258.py:224: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_ckpt = torch.load(\n",
      "/tmp/ipykernel_4192/3514950170.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data_dict = torch.load(self.path + file) # load data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 7 target classes\n",
      "Got 7 prediction classes\n",
      "Resulting in 7 total classes\n",
      "['DJI', 'FutabaT14', 'FutabaT7', 'Graupner', 'Noise', 'Taranis', 'Turnigy']\n",
      "Test accuracy: 0.9433643221855164 Test weighted accuracy: 0.9197680950164795 Best epoch: 0\n"
     ]
    }
   ],
   "source": [
    "project_path = './'\n",
    "result_path = project_path + 'results/experiments/'\n",
    "data_path = './data/drone_RF_data/'\n",
    "\n",
    "# global params\n",
    "num_workers = 0 # number of workers for data loader\n",
    "num_folds = 5 # number of folds for cross validation\n",
    "num_epochs = 40 # number of epochs to train\n",
    "batch_size = 2 # batch size\n",
    "learning_rate = 0.001 # start learning rate\n",
    "train_verbose = True  # show epoch\n",
    "model_name = 'vgg11_bn'\n",
    "\n",
    "# set device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "\n",
    "experiment_name = model_name + \\\n",
    "                '_CV' + str(num_folds) +\\\n",
    "                '_epochs' + str(num_epochs) + \\\n",
    "                '_lr' + str(learning_rate) + \\\n",
    "                '_batchsize' + str(batch_size)\n",
    "\n",
    "\n",
    "print('Starting experiment:', experiment_name)\n",
    "\n",
    "# create path to store results\n",
    "act_result_path = result_path + 'vgg11_bn_CV5_epochs50_lr0.001_batchsize2' + '/'\n",
    "try:\n",
    "    os.mkdir(act_result_path)\n",
    "except OSError as error:\n",
    "    print(error)\n",
    "try:\n",
    "    os.mkdir(act_result_path + 'plots/')\n",
    "except OSError as error:\n",
    "    print(error)\n",
    "\n",
    "# read statistics/class count of the dataset\n",
    "dataset_stats = pd.read_csv(data_path + 'class_stats.csv', index_col=0)\n",
    "class_names = dataset_stats['class'].values\n",
    "\n",
    "# read SNR count of the dataset\n",
    "snr_stats = pd.read_csv(data_path + 'SNR_stats.csv', index_col=0)\n",
    "snr_list = snr_stats['SNR'].values\n",
    "\n",
    "# setup transform: IQ -> SPEC\n",
    "data_transform = transform_spectrogram(device=device) # create transform object\n",
    "# create dataset object\n",
    "drone_dataset = drone_data_dataset(path=data_path, device=device, transform=data_transform)\n",
    "\n",
    "# split data with stratified kfold\n",
    "dataset_indices = list(range(len(drone_dataset)))\n",
    "\n",
    "# targets = drone_dataset.get_targets()\n",
    "# snr_list = drone_dataset.get_snrs()\n",
    "# files = drone_dataset.get_files()\n",
    "\n",
    "# fold=0\n",
    "# here change the fold [0] from 0-4 on different machine each\n",
    "for fold in [0]:\n",
    "    print('Fold:', fold)\n",
    "    # Tensorboard writer will output to ./runs/ directory by default\n",
    "    writer = SummaryWriter(act_result_path + 'runs/fold' + str(fold))\n",
    "\n",
    "    # split data with stratified kfold with respect to target class\n",
    "    train_idx, test_idx = train_test_split(dataset_indices, test_size=1/num_folds, stratify=drone_dataset.get_targets())\n",
    "    y_test = [drone_dataset.get_targets()[x] for x in test_idx]\n",
    "    y_train = [drone_dataset.get_targets()[x] for x in train_idx]\n",
    "\n",
    "    # split val data from train data in stratified k-fold manner\n",
    "    train_idx, val_idx = train_test_split(train_idx, test_size=1/num_folds, stratify=y_train)\n",
    "    y_val = [drone_dataset.get_targets()[x] for x in val_idx]\n",
    "    y_train = [drone_dataset.get_targets()[x] for x in train_idx]\n",
    "\n",
    "    # get train samples weight by class weight for each train target\n",
    "    class_weights = 1. / dataset_stats['count']\n",
    "\n",
    "    train_samples_weight = np.array([class_weights[int(i)] for i in y_train])\n",
    "    train_samples_weight = torch.from_numpy(train_samples_weight)\n",
    "\n",
    "    train_dataset = torch.utils.data.Subset(drone_dataset, train_idx)\n",
    "    val_dataset = torch.utils.data.Subset(drone_dataset, val_idx)\n",
    "    test_dataset = torch.utils.data.Subset(drone_dataset, test_idx)\n",
    "\n",
    "    # define weighted random sampler with the weighted train samples\n",
    "    train_sampler = torch.utils.data.WeightedRandomSampler(train_samples_weight.type('torch.DoubleTensor'), len(train_samples_weight))\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        sampler=train_sampler,\n",
    "        num_workers=num_workers, \n",
    "        pin_memory=False)\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=False)\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        dataset=test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=False)\n",
    "\n",
    "    dataloaders = {'train': train_loader, 'val': val_loader, 'test': test_loader}\n",
    "    dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset), 'test': len(test_dataset)}\n",
    "\n",
    "    num_classes = len(np.unique(y_val))\n",
    "\n",
    "    model = get_model_spec(model_name, num_classes)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # criterion = nn.CrossEntropyLoss(weight=torch.Tensor(class_weights).to(device))\n",
    "    criterion = nn.CrossEntropyLoss()  # don't use class weights in the loss\n",
    "\n",
    "    # Observe that all parameters are being optimized\n",
    "    # optimizer_ft = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    optimizer_ft = optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=learning_rate,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-08,\n",
    "        weight_decay=1e-4,   # ← ADD THIS (L2 regularization)\n",
    "        amsgrad=False\n",
    "    )\n",
    "    \n",
    "    # Add scheduler to reduce LR when val loss plateaus\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer_ft,\n",
    "        mode='min',          # minimize val loss\n",
    "        factor=0.5,          # reduce LR by half\n",
    "        patience=3,          # wait 3 epochs with no improvement\n",
    "        verbose=True         # print when LR changes\n",
    "    )\n",
    "    # Decay LR by a factor of 0.1 every 7 epochs\n",
    "    # exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
    "    # plateau_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, mode='min', factor=0.1, patience=3, threshold=0.0001,\n",
    "                                                        # threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08, verbose=True)\n",
    "\n",
    "    # --- CHECKPOINT RESUME SETUP ---\n",
    "    resume_checkpoint = \"/home/ghoth/thesis_drone_detection/Robust-Drone-Detection-and-Classification/results/experiments/vgg11_bn_CV5_epochs50_lr0.001_batchsize2/checkpoint_fold0_epoch39.pth\"  # Change to a path like 'checkpoint_fold0_epoch1.pth' to resume\n",
    "\n",
    "    # Initialize lists FIRST (important!)\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    val_loss = []\n",
    "    val_acc = []\n",
    "    train_weighted_acc = []\n",
    "    val_weighted_acc = []\n",
    "    lr = []  # ← Define lr here before using it\n",
    "\n",
    "    start_epoch = 0\n",
    "    if resume_checkpoint is not None and os.path.exists(resume_checkpoint):\n",
    "        print(f\"Resuming training from checkpoint: {resume_checkpoint}\")\n",
    "        checkpoint = torch.load(resume_checkpoint, map_location=device)\n",
    "        \n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer_ft.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        \n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        \n",
    "        # Restore metric lists for continuous plots\n",
    "        train_loss = checkpoint.get('train_loss', [])\n",
    "        train_acc = checkpoint.get('train_acc', [])\n",
    "        val_loss = checkpoint.get('val_loss', [])\n",
    "        val_acc = checkpoint.get('val_acc', [])\n",
    "        train_weighted_acc = checkpoint.get('train_weighted_acc', [])\n",
    "        val_weighted_acc = checkpoint.get('val_weighted_acc', [])\n",
    "        lr = checkpoint.get('lr', lr)  # fallback to current\n",
    "        best_acc = checkpoint.get('best_acc', 0.0)  # Restore best acc if saved, else start at 0\n",
    "        \n",
    "        print(f\"Resumed from epoch {checkpoint['epoch']}. Starting at epoch {start_epoch}\")\n",
    "    else:\n",
    "        print(\"Starting training from scratch\")\n",
    "        # Initialize empty lists if fresh start\n",
    "        train_loss = []\n",
    "        train_acc = []\n",
    "        val_loss = []\n",
    "        val_acc = []\n",
    "        train_weighted_acc = []\n",
    "        val_weighted_acc = []\n",
    "        lr = []\n",
    "        best_acc = 0.0\n",
    "    # --- END CHECKPOINT SETUP ---\n",
    "                                                                                                    \n",
    "    # train model\n",
    "    model, train_loss, train_acc, val_loss, val_acc, train_weighted_acc, val_weighted_acc, best_epoch, lr = train_model_observe_snr_performance_spec(model=model,\n",
    "                                                                                                                        criterion=criterion,\n",
    "                                                                                                                        optimizer=optimizer_ft,\n",
    "                                                                                                                        scheduler=None,\n",
    "                                                                                                                        num_classes=num_classes,\n",
    "                                                                                                                        num_epochs=num_epochs,\n",
    "                                                                                                                        snr_list_for_observation=[0, -10, -20])\n",
    "\n",
    "    # show/store learning curves\n",
    "    plt.plot(train_loss)\n",
    "    plt.plot(val_loss)\n",
    "    plt.legend(['train', 'val'])\n",
    "    plt.title('Loss')\n",
    "    plt.savefig(act_result_path + 'plots/loss_fold' + str(fold) + '.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.plot(train_acc)\n",
    "    plt.plot(val_acc)\n",
    "    plt.legend(['train', 'val'])\n",
    "    plt.title('Acc')\n",
    "    plt.savefig(act_result_path + 'plots/acc_fold' + str(fold) + '.png')\n",
    "    plt.close()\n",
    "    # plt.show()\n",
    "\n",
    "    plt.plot(train_weighted_acc)\n",
    "    plt.plot(val_weighted_acc)\n",
    "    plt.legend(['train', 'val'])\n",
    "    plt.title('Weighted Acc')\n",
    "    plt.savefig(act_result_path + 'plots/weigthed_acc_fold' + str(fold) + '.png')\n",
    "    plt.close()\n",
    "    # plt.show()\n",
    "\n",
    "   # ---- LOAD BEST MODEL ----\n",
    "    best_ckpt = torch.load(\n",
    "        act_result_path + f'best_model_fold{fold}.pth',\n",
    "        map_location=device\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(best_ckpt['model_state_dict'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # ---- EVALUATE ON TEST SET ----\n",
    "    eval_acc, eval_weighted_acc, eval_predictions, eval_targets, eval_snrs, eval_duty_cycle = eval_model_spec(\n",
    "        model=model,\n",
    "        num_classes=num_classes,\n",
    "        data_loader=dataloaders['test']\n",
    "    )\n",
    "\n",
    "    eval_targets = eval_targets.cpu()\n",
    "    eval_predictions = eval_predictions.cpu()\n",
    "    eval_snrs = eval_snrs.cpu()\n",
    "    target_classes = np.unique(eval_targets)\n",
    "    pred_classes = np.unique(eval_predictions)\n",
    "    eval_classes = np.union1d(target_classes, pred_classes)\n",
    "    eval_class_names = [class_names[int(x)] for x in eval_classes]\n",
    "\n",
    "    print('Got ' + str(len(target_classes)) + ' target classes')\n",
    "    print('Got ' + str(len(pred_classes)) + ' prediction classes')\n",
    "    print('Resulting in ' + str(len(eval_classes)) + ' total classes')\n",
    "    print(eval_class_names)\n",
    "\n",
    "    print('Test accuracy:', eval_acc, 'Test weighted accuracy:', eval_weighted_acc, 'Best epoch:', best_epoch)\n",
    "\n",
    "    save_dict = {'train_weighted_acc': train_weighted_acc,\n",
    "                    'train_acc': train_acc,\n",
    "                    'train_loss': train_loss,\n",
    "                    'val_weighted_acc': val_weighted_acc,\n",
    "                    'val_acc': val_acc,\n",
    "                    'val_loss': val_loss,\n",
    "                    'best_epoch': best_epoch,\n",
    "                    'test_acc': eval_acc,\n",
    "                    'test_weighted_acc': eval_weighted_acc,\n",
    "                    'test_predictions': eval_predictions,\n",
    "                    'test_targets': eval_targets,\n",
    "                    'test_snrs': eval_snrs,\n",
    "                    'class_names': class_names,\n",
    "                    'train_idx': train_idx,\n",
    "                    'val_idx': val_idx,\n",
    "                    'test_idx': test_idx\n",
    "                    }\n",
    "    save_filename = 'results_fold' + str(fold) + '.pkl'\n",
    "\n",
    "    outfile = open(act_result_path + save_filename, 'wb')\n",
    "    pkl.dump(save_dict, outfile)\n",
    "    outfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_environment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
